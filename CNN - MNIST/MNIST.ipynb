{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Dataset Loading ===\n",
    "\n",
    "transform = transforms.ToTensor() # Used to convert images into matrices/tensors\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform) \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True) # Batch is the data size the CNN is trained on before weight updating, shuffle is set to True so the model does not learn the order of the data and generalizes better\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Random Image Plot ===\n",
    "image, label = random.choice(trainset)\n",
    "plt.imshow(image[0], cmap='gray') \n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Model ===\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self): # __init__ is a function that defines the layers (building blocks) of the model\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) # 1 input channel because black/white image\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2) # First 2 represents the window size i.e. 2x2, Second 2 represents how far the window moves\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.dropout_fc = nn.Dropout(0.3) # Randomly drops 30% of neurons during training to reduce overfitting\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x): # forward is a function that defines how data flows through those layers (the order of operations)\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train ===\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = CustomCNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10): # Train for 10 epochs to improve model performance\n",
    "    for images, labels in trainloader: # Grabbing both the batch of image tensors and their labels\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images) # Model makes predictions for the batch\n",
    "        loss = loss_fn(outputs, labels) # Compare predictions to actual labels to calculate loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Backpropagation\n",
    "        optimizer.step() # Updates the models weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.30%\n"
     ]
    }
   ],
   "source": [
    "# === Evaluate ===\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "684ff43e33061ad1880c5801bcaf42e322e7f83bffbde2c4381d6344dc2d1c33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
